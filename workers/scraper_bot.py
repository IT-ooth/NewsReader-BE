from scrapers import BoanNewsScraper, GeekNewsScraper
from analyzers import OllamaAnalyzer
from db import services, engine, init_db

import time
from sqlmodel import Session

def run_curation_loop():
    # 1. ì´ˆê¸°í™” (DB í…Œì´ë¸” ìƒì„± ë° ì—”ì§„ ì¤€ë¹„)
    init_db()
    
    # 2. ë¶€í’ˆ ì¤€ë¹„
    # ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±
    scrapers = [
        BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=1'),
        # BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=5'),
        # BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=7'),
        # BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=3'),
        # BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=2'),
        # BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=6'),
        GeekNewsScraper('https://news.hada.io/rss/news'),
    ]
    analyzer = OllamaAnalyzer(model_name="llama3.1:8b")

    print("ğŸš€ ë³´ì•ˆ ë‰´ìŠ¤ íë ˆì´ì…˜ ë´‡ ì‹œì‘...")

    while True:
        print(f"\n[ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}] ì‘ì—… ì‹œì‘...")
        
        with Session(engine) as session:
            for scraper in scrapers:
                try:
                    scraped_items = scraper.collect(session) 
                    
                    for item in scraped_items:
                        if services.is_already_analyzed(session, item.url):
                            continue
                            
                        print(f"ğŸ“° ì²˜ë¦¬ ì¤‘: {item.title}")

                        db_article = services.get_article_by_url(session, item.url)
                        if not db_article:
                            db_article = services.save_article(session, item)
                    
                        try:
                            print(f"ğŸ¤– AI ë¶„ì„ ì¤‘...")
                            analysis_data = analyzer.analyze(item) 
                            
                            if analysis_data:
                                services.save_analysis(session, db_article.id, analysis_data)
                                print(f"âœ… ë¶„ì„ ì™„ë£Œ ë° ì €ì¥ ì„±ê³µ")
                        except Exception as e:
                            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨ (ID: {db_article.id}): {e}")

                except Exception as e:
                    print(f"âŒ ìŠ¤í¬ë˜í¼ ì˜¤ë¥˜: {e}")

        print("\nğŸ’¤ ëŒ€ê¸° ì¤‘ (10ë¶„ ë’¤ ë‹¤ì‹œ í™•ì¸)...")
        time.sleep(600)

if __name__ == "__main__":
    run_curation_loop()