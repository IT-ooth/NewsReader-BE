from scrapers import BoanNewsScraper, GeekNewsScraper
from analyzers import OllamaAnalyzer
from db import services, engine, init_db

import time
from sqlmodel import Session

def run_curation_loop():
    # 1. ì´ˆê¸°í™” (DB í…Œì´ë¸” ìƒì„± ë° ì—”ì§„ ì¤€ë¹„)
    init_db()
    
    # 2. ë¶€í’ˆ ì¤€ë¹„
    # ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±
    scrapers = [
        BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=1'),
        # BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=5'),
        # BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=7'),
        # BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=3'),
        # BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=2'),
        # BoanNewsScraper('http://www.boannews.com/media/news_rss.xml?skind=6'),
        # GeekNewsScraper('https://news.hada.io/rss/news'),
    ]
    analyzer = OllamaAnalyzer(model_name="llama3.1:8b")

    print("ğŸš€ ë³´ì•ˆ ë‰´ìŠ¤ íë ˆì´ì…˜ ë´‡ ì‹œì‘...")

    while True:
        print(f"\n[ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}] ìƒˆë¡œìš´ ë‰´ìŠ¤ í™•ì¸ ì¤‘...")
        
        with Session(engine) as session:
            for scraper in scrapers:
                try:
                    new_articles = scraper.collect(session)
                    
                    for article_item in new_articles:
                        print(f"ğŸ“° ìƒˆ ê¸°ì‚¬ ë°œê²¬: {article_item.title}")
                        
                        try:
                            db_article = services.save_article(session, article_item)
                            
                            print(f"ğŸ¤– AI ë¶„ì„ ì¤‘... ({article_item.title[:20]}...)")
                            analysis_data = analyzer.analyze(article_item)
                            
                            if analysis_data:
                                services.save_analysis(session, db_article.id, analysis_data)
                                print(f"âœ… ë¶„ì„ ì™„ë£Œ ë° ì €ì¥ ì„±ê³µ")
                            else:
                                print(f"âš ï¸ ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (ê¸°ì‚¬ ID: {db_article.id})")
                                
                        except Exception as e:
                            print(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ (ê±´ë„ˆëœ€): {e}")
                            continue # ë‹¤ìŒ ê¸°ì‚¬ë¡œ ë„˜ì–´ê°
                            
                except Exception as e:
                    print(f"âŒ ìŠ¤í¬ë˜í¼({scraper.__class__.__name__}) ì‘ë™ ì˜¤ë¥˜: {e}")
        
        print("\nğŸ’¤ ëŒ€ê¸° ì¤‘ (10ë¶„ ë’¤ ë‹¤ì‹œ í™•ì¸)...")
        time.sleep(6000)

if __name__ == "__main__":
    run_curation_loop()